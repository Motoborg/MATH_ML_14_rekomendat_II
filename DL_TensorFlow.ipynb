{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %conda install tensorflow\n",
    "# %conda install -c conda-forge lightfm\n",
    "# %conda install -c conda-forge tensorflow\n",
    "# !pip install tensorflow tensorboardX # Это помогло"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from lightfm import LightFM\n",
    "from lightfm.cross_validation import random_train_test_split\n",
    "from lightfm.evaluation import precision_at_k, recall_at_k  \n",
    "\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dot, Dense, Concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Gooddreadbooks/ratings.csv') # Поставленные оценки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785404, 3)\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "53424\n"
     ]
    }
   ],
   "source": [
    "# Предварительно вычислим количество уникальных пользователей и книг:\n",
    "n_books = df[\"book_id\"].nunique()\n",
    "print(n_books)\n",
    "n_users = df[\"user_id\"].nunique()\n",
    "print(n_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В первую очередь нам необходимо создать эмбеддинги для книг и пользователей. Создаём эмбеддинги для книг:\n",
    "book_input = Input(shape=[1], name=\"Book-Input\")\n",
    "book_embedding = Embedding(n_books+1, 5, name=\"Book-Embedding\")(book_input)\n",
    "book_vec = Flatten(name=\"Flatten-Books\")(book_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы задаём размерность входного слоя. После этого определяем размер эмбеддинга — в данном случае снижаем размерность до 5. Далее мы разворачиваем результат в массив с одним измерением с помощью слоя Flatten().\n",
    "\n",
    "Делаем то же самое для пользователей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape=[1], name=\"User-Input\")\n",
    "user_embedding = Embedding(n_users+1, 5, name=\"User-Embedding\")(user_input)\n",
    "user_vec = Flatten(name=\"Flatten-Users\")(user_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь, когда мы создали представления как для книг, так и для пользователей, нам необходимо соединить их:\n",
    "conc = Concatenate()([book_vec, user_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее начинаем «собирать» нашу нейронную сеть из слоёв. Dense обозначает полносвязный слой. \n",
    "\n",
    "Также мы обозначаем для него количество нейронов и данные, которые идут на вход."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1 = Dense(128, activation='relu')(conc)\n",
    "fc2 = Dense(32, activation='relu')(fc1)\n",
    "out = Dense(1)(fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Собираем модель — передаём входные данные для книг и пользователей, а также архитектуру нейронной сети:\n",
    "model2 = Model([user_input, book_input], out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также нам необходимо задать алгоритм оптимизации и метрику, которую мы будем оптимизировать. В данном случае будем использовать метод adam и хорошо известную вам среднеквадратичную ошибку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer = 'adam',loss =  'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "24544/24544 [==============================] - 100s 4ms/step - loss: 0.7943\n",
      "Epoch 2/5\n",
      "24544/24544 [==============================] - 102s 4ms/step - loss: 0.6875\n",
      "Epoch 3/5\n",
      "24544/24544 [==============================] - 103s 4ms/step - loss: 0.6545\n",
      "Epoch 4/5\n",
      "24544/24544 [==============================] - 98s 4ms/step - loss: 0.6286\n",
      "Epoch 5/5\n",
      "24544/24544 [==============================] - 101s 4ms/step - loss: 0.6069\n"
     ]
    }
   ],
   "source": [
    "# Теперь будем обучать нашу модель:\n",
    "history = model2.fit([train.user_id, train.book_id], train.rating, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6136/6136 [==============================] - 8s 1ms/step - loss: 0.7114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7113780975341797"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Теперь можно оценить качество:\n",
    "model2.evaluate([test.user_id, test.book_id], test.rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Примечание.** К сожалению, результаты этого алгоритма нельзя зафиксировать стандартным ramdom_state, к которому мы привыкли: применяемые методы не используют такой параметр. Поэтому мы опустим здесь сравнение результатов, однако посмотрим, как можно настроить нейронную сеть."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обычно для улучшения качества модели каким-то образом модифицируют нейронную сеть: дополняют её, увеличивают время обучения. Добавим ещё один полносвязный слой с восемью нейронами после полносвязного слоя с 32 нейронами. Обучим нейронную сеть, реализовав десять эпох:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24544/24544 [==============================] - 104s 4ms/step - loss: 0.6196\n",
      "Epoch 2/10\n",
      "24544/24544 [==============================] - 107s 4ms/step - loss: 0.5739\n",
      "Epoch 3/10\n",
      "24544/24544 [==============================] - 108s 4ms/step - loss: 0.5537\n",
      "Epoch 4/10\n",
      "24544/24544 [==============================] - 105s 4ms/step - loss: 0.5361\n",
      "Epoch 5/10\n",
      "24544/24544 [==============================] - 104s 4ms/step - loss: 0.5218\n",
      "Epoch 6/10\n",
      "24544/24544 [==============================] - 100s 4ms/step - loss: 0.5100\n",
      "Epoch 7/10\n",
      "24544/24544 [==============================] - 106s 4ms/step - loss: 0.5002\n",
      "Epoch 8/10\n",
      "24544/24544 [==============================] - 96s 4ms/step - loss: 0.4919\n",
      "Epoch 9/10\n",
      "24544/24544 [==============================] - 93s 4ms/step - loss: 0.4845\n",
      "Epoch 10/10\n",
      "24544/24544 [==============================] - 97s 4ms/step - loss: 0.4782\n",
      "6136/6136 [==============================] - 7s 1ms/step - loss: 0.7778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7778374552726746"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc1 = Dense(128, activation='relu')(conc)\n",
    "fc2 = Dense(32, activation='relu')(fc1)\n",
    "fc3 = Dense(8, activation='relu')(fc2)\n",
    "out = Dense(1)(fc3)\n",
    "\n",
    "model2 = Model([user_input, book_input], out)\n",
    "model2.compile('adam', 'mean_squared_error')\n",
    "result = model2.fit([train.user_id, train.book_id], train.rating, epochs=10, verbose=1)\n",
    "model2.evaluate([test.user_id, test.book_id], test.rating)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
